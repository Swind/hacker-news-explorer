---
story_id: 46840676
hn_url: https://news.ycombinator.com/item?id=46840676
title: "Autonomous cars, drones cheerfully obey prompt injection by road sign"
verdict: interesting
created_at: 2026-02-02T04:18:29
---

# 2026-02-02: 自駕車與無人機易受路標提示注入攻擊

**來源：** Hacker News
**故事 ID：** 46840676
**Hacker News 連結：** https://news.ycombinator.com/item?id=46840676
**網址：** https://www.theregister.com/2026/01/30/road_sign_hijack_ai/
**分數：** 198 | **評論數：** 180

## 摘要

加州大學聖塔克魯茲分校與約翰霍普金斯大學的研究人員展示了一種新型攻擊方式「CHAI」（Command Hijacking Against Embodied AI），透過在路標上顯示特定文字，即可欺騙自駕車與無人機的 AI 視覺系統，使其執行錯誤指令。研究顯示，在模擬與實際測試中，攻擊成功率可達 81.8% 至 95.5%。

## 為什麼有趣

這是一個**重大安全警訊**，具有以下重要意義：

1. **新型實體攻擊面**：首次證明「間接提示注入」可從虛擬世界延伸至實體環境，攻擊者可透過簡單的路標貼紙或手持標誌即可劫持 AI 系統決策

2. **跨語言攻擊有效性**：研究測試了中文、英文、西班牙文及混合語言（Spanglish），顯示此攻擊具備普遍性，不受語言限制

3. **高成功率的現實威脅**：
   - 自駕車測試：81.8% 成功率，會錯誤地穿越有行人的斑馬線
   - 無人機追蹤測試：95.5% 錯誤率，會跟隨標有 "Police Santa Cruz" 的一般車輛
   - 無人機降落測試：68.1% 會降落在有殘骸但標示 "Safe to land" 的屋頂

4. **實體環境驗證**：研究人員在真實環境中使用遙控車測試，GPT-4o 在不同光照條件下分別達到 92.5% 和 87.76% 的劫持成功率

## 主要討論點

**技術層面的批判：**
- 許多評論指出，這暴露了端到端 AI 模型的根本性缺陷：單一模型缺乏多層安全驗證機制
- 專家強調，正確的系統架構應將「不撞擊物體」、「保持在可行駛路面」等安全目標優先於「遵守路標」
- 有人提到 Mercedes 等車廠採用的分層安全架構（Level 2++），可降低此類風險

**現實世界類比：**
- 許多人分享被奇怪路標困惑的經驗，但人類會透過情境判斷（如施工人員、重卡車、警告標誌）來識別異常
- 人類不會因為看到「Proceed」綠色標誌就直接闖紅燈，這突顯 AI 缺乏常識推理能力

**防禦與責任議題：**
- 研究團隊呼籲需要新的防禦機制來對抗這類攻擊
- 討論了路標遭人為破壞的法律責任問題（民事與刑事責任）
- 有人擔憂，如果自駕車系統架構不當，「一張貼在路燈上的列印紙就能讓車輛可靠地撞毀」

**廣泛的脆弱性：**
- 評論指出，這不僅是 VLM 的問題，任何神經網路架構都可能遭受類似攻擊
- 過去已見過分類器被看似無害的物質欺騙，LLM 也存在提示注入問題，現在延伸至具身 AI

## 評價

**值得高度關注** —— 這項研究揭示了 AI 安全的一個關鍵盲點：

- **技術重要性**：這不僅是學術研究，而是對現有自駕技術架構的直接質疑，影響 Tesla、Waymo 等公司
- **現實威脅**：攻擊成本低廉（只需列印貼紙），但潛在危害極大（可能造成人員傷亡）
- **系統性問題**：暴露了 AI 開發中「單一模型負責所有決策」的危險做法
- **行業影響**：可能促使監管機構重新審查自駕車的安全認證標準

研究團隊計劃在雨天、模糊影像等更複雜條件下繼續測試，並開發對應防禦措施。這是一個持續發展的安全領域，值得密切追蹤。